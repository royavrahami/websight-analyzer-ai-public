#!/usr/bin/env python3
"""
Automated QA Orchestrator
=========================

Professional QA automation orchestrator that generates comprehensive test suites
automatically from web analysis data. Integrates with GUI applications and provides
intelligent test generation using AI-powered analysis.

Key Features:
- Automated test suite generation (Functional, Negative, API, UI, Accessibility)
- Multi-page analysis support
- Integration with popular testing frameworks (pytest, playwright)
- Performance and load testing generation
- AI-powered test scenarios
- Professional reporting and documentation

Author: Roy Avrahami - Senior QA Automation Architect
"""

import asyncio
import json
import logging
import os
import threading
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable, Union
import random
import hashlib

# Try to import AI libraries for enhanced test generation
try:
    import anthropic
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False


class QATestConfig:
    """Configuration for QA test generation"""
    
    def __init__(self, test_counts: Optional[Dict[str, int]] = None):
        self.test_counts = test_counts or {
            'functional': 5,
            'negative': 5,
            'api': 5,
            'ui': 5,
            'accessibility': 5
        }
        self.include_performance = True
        self.include_load_tests = True
        self.include_security_tests = True
        self.use_ai_enhancement = AI_AVAILABLE
        
    def get_total_tests(self) -> int:
        """Get total number of tests to generate"""
        return sum(self.test_counts.values())


class QATestGenerator:
    """Generates various types of automated tests"""
    
    def __init__(self, config: QATestConfig, log_callback: Optional[Callable[[str], None]] = None):
        self.config = config
        self.log_callback = log_callback or print
        
    def log_message(self, message: str):
        """Log message using callback"""
        self.log_callback(f"[QA-GEN] {message}")
        
    def generate_functional_tests(self, analysis_data: Dict[str, Any], output_dir: Path) -> List[str]:
        """Generate functional test cases"""
        self.log_message(f"üîß Generating {self.config.test_counts['functional']} functional tests...")
        
        tests = []
        test_file = output_dir / "test_functional.py"
        
        # Read existing analysis data
        elements_data = self._load_elements_data(analysis_data)
        
        test_content = f'''"""
Functional Test Suite
====================
Auto-generated functional tests for comprehensive page validation.

Generated: {datetime.now().isoformat()}
Test Count: {self.config.test_counts['functional']}
"""

import pytest
from playwright.sync_api import Page, expect
import time


class TestFunctionalSuite:
    """Comprehensive functional test suite"""
    
    def test_page_loads_successfully(self, page: Page):
        """Test that the page loads without errors"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        # Verify page loads
        expect(page).to_have_title(lambda title: len(title) > 0)
        
        # Check for basic page structure
        expect(page.locator("body")).to_be_visible()
        
        print("‚úÖ Page loads successfully")
    
    def test_critical_elements_present(self, page: Page):
        """Test that critical page elements are present"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        # Test main navigation elements
        navigation_selectors = ["nav", "[role='navigation']", ".navbar", ".menu"]
        navigation_found = False
        
        for selector in navigation_selectors:
            try:
                if page.locator(selector).count() > 0:
                    navigation_found = True
                    print(f"‚úÖ Navigation found: {{selector}}")
                    break
            except:
                continue
                
        # Test main content area
        content_selectors = ["main", "[role='main']", ".main", ".content", "article"]
        content_found = False
        
        for selector in content_selectors:
            try:
                if page.locator(selector).count() > 0:
                    content_found = True
                    print(f"‚úÖ Main content found: {{selector}}")
                    break
            except:
                continue
        
        assert navigation_found or content_found, "Neither navigation nor main content found"
    
    def test_interactive_elements_functional(self, page: Page):
        """Test that interactive elements function correctly"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        # Test buttons
        buttons = page.locator("button, [role='button'], input[type='submit']").all()
        button_count = 0
        
        for button in buttons[:3]:  # Test first 3 buttons
            try:
                if button.is_visible():
                    button_count += 1
                    button.click(timeout=2000)
                    print(f"‚úÖ Button {{button_count}} clicked successfully")
                    time.sleep(0.5)
            except:
                print(f"‚ö†Ô∏è Button {{button_count}} not clickable")
        
        if button_count > 0:
            print(f"‚úÖ {{button_count}} interactive elements tested")
        else:
            print("‚ÑπÔ∏è No interactive elements found to test")
    
    def test_form_validation(self, page: Page):
        """Test form validation and input handling"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        # Find forms
        forms = page.locator("form").all()
        forms_tested = 0
        
        for form in forms[:2]:  # Test first 2 forms
            try:
                # Find text inputs in the form
                text_inputs = form.locator("input[type='text'], input[type='email'], textarea").all()
                
                for input_field in text_inputs[:2]:  # Test first 2 inputs per form
                    if input_field.is_visible():
                        # Test valid input
                        input_field.fill("test@example.com")
                        forms_tested += 1
                        print(f"‚úÖ Form input {{forms_tested}} filled successfully")
                        
            except Exception as e:
                print(f"‚ö†Ô∏è Form testing error: {{e}}")
        
        if forms_tested > 0:
            print(f"‚úÖ {{forms_tested}} form inputs tested")
        else:
            print("‚ÑπÔ∏è No forms found to test")
    
    def test_page_performance_basic(self, page: Page):
        """Test basic page performance metrics"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        
        start_time = time.time()
        page.goto(url, wait_until="load")
        load_time = (time.time() - start_time) * 1000
        
        # Assert reasonable load time (5 seconds max)
        assert load_time < 5000, f"Page load time too slow: {{load_time:.1f}}ms"
        
        print(f"‚úÖ Page loaded in {{load_time:.1f}}ms")
'''
        
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_content)
            
        tests.append(str(test_file))
        self.log_message(f"‚úÖ Functional tests generated: {test_file}")
        return tests
    
    def generate_negative_tests(self, analysis_data: Dict[str, Any], output_dir: Path) -> List[str]:
        """Generate negative test cases"""
        self.log_message(f"‚ùå Generating {self.config.test_counts['negative']} negative tests...")
        
        tests = []
        test_file = output_dir / "test_negative.py"
        
        test_content = f'''"""
Negative Test Suite
==================
Auto-generated negative tests for edge case and error condition validation.

Generated: {datetime.now().isoformat()}
Test Count: {self.config.test_counts['negative']}
"""

import pytest
from playwright.sync_api import Page, expect
import time


class TestNegativeSuite:
    """Comprehensive negative test suite"""
    
    def test_invalid_url_handling(self, page: Page):
        """Test handling of invalid or non-existent URLs"""
        base_url = "{analysis_data.get('url', 'https://example.com')}"
        invalid_urls = [
            f"{{base_url}}/nonexistent-page-{random.randint(1000, 9999)}",
            f"{{base_url}}/404-test",
            f"{{base_url}}/invalid-path"
        ]
        
        for invalid_url in invalid_urls:
            try:
                response = page.goto(invalid_url, wait_until="load", timeout=10000)
                
                # Check if page handles 404 gracefully
                if response and response.status == 404:
                    print(f"‚úÖ 404 handled correctly for: {{invalid_url}}")
                else:
                    print(f"‚ö†Ô∏è Unexpected response for invalid URL: {{response.status if response else 'No response'}}")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Error testing invalid URL {{invalid_url}}: {{e}}")
    
    def test_malformed_input_handling(self, page: Page):
        """Test handling of malformed or malicious input"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        # Test malformed inputs
        malformed_inputs = [
            "<script>alert('xss')</script>",
            "'; DROP TABLE users; --",
            "../../../../etc/passwd",
            "javascript:alert('test')",
            "data:text/html,<script>alert('test')</script>"
        ]
        
        forms = page.locator("form").all()
        inputs_tested = 0
        
        for form in forms[:1]:  # Test first form only
            text_inputs = form.locator("input[type='text'], input[type='email'], textarea").all()
            
            for input_field in text_inputs[:2]:  # Test first 2 inputs
                for malformed in malformed_inputs[:3]:  # Test first 3 malformed inputs
                    try:
                        if input_field.is_visible():
                            input_field.fill(malformed)
                            inputs_tested += 1
                            
                            # Submit form if submit button available
                            submit_btn = form.locator("input[type='submit'], button[type='submit']").first
                            if submit_btn.count() > 0:
                                submit_btn.click(timeout=2000)
                            
                            print(f"‚úÖ Malformed input tested: {{malformed[:20]}}...")
                            time.sleep(0.5)
                            
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error testing malformed input: {{e}}")
        
        if inputs_tested > 0:
            print(f"‚úÖ {{inputs_tested}} malformed inputs tested")
        else:
            print("‚ÑπÔ∏è No forms found for negative input testing")
    
    def test_rapid_consecutive_actions(self, page: Page):
        """Test system behavior under rapid consecutive actions"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        # Find clickable elements
        clickable_elements = page.locator("button, [role='button'], a").all()
        
        if clickable_elements:
            # Test rapid clicking on first element
            element = clickable_elements[0]
            clicks_performed = 0
            
            try:
                for i in range(5):  # Rapid clicks
                    if element.is_visible():
                        element.click(timeout=1000)
                        clicks_performed += 1
                    time.sleep(0.1)  # Very short delay
                    
                print(f"‚úÖ {{clicks_performed}} rapid clicks performed")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error during rapid clicking: {{e}}")
        else:
            print("‚ÑπÔ∏è No clickable elements found for rapid action testing")
    
    def test_session_boundary_conditions(self, page: Page):
        """Test behavior at session boundaries and limits"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        
        # Test with cleared storage
        page.context.clear_cookies()
        page.goto(url)
        
        # Verify page still loads correctly without cookies
        expect(page.locator("body")).to_be_visible()
        print("‚úÖ Page loads correctly without cookies")
        
        # Test with disabled JavaScript (if possible)
        try:
            page.context.add_init_script("window.JavaScript = undefined;")
            page.goto(url)
            expect(page.locator("body")).to_be_visible()
            print("‚úÖ Page handles JavaScript restrictions")
        except:
            print("‚ö†Ô∏è Could not test JavaScript restrictions")
    
    def test_extreme_input_lengths(self, page: Page):
        """Test handling of extremely long input values"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        # Create extremely long input
        long_input = "A" * 10000  # 10KB of text
        
        forms = page.locator("form").all()
        long_inputs_tested = 0
        
        for form in forms[:1]:  # Test first form
            text_inputs = form.locator("input[type='text'], textarea").all()
            
            for input_field in text_inputs[:1]:  # Test first input
                try:
                    if input_field.is_visible():
                        input_field.fill(long_input)
                        long_inputs_tested += 1
                        print(f"‚úÖ Extreme length input tested: {{len(long_input)}} characters")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Error testing long input: {{e}}")
        
        if long_inputs_tested == 0:
            print("‚ÑπÔ∏è No suitable inputs found for extreme length testing")
'''
        
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_content)
            
        tests.append(str(test_file))
        self.log_message(f"‚úÖ Negative tests generated: {test_file}")
        return tests
    
    def generate_accessibility_tests(self, analysis_data: Dict[str, Any], output_dir: Path) -> List[str]:
        """Generate accessibility test cases"""
        self.log_message(f"‚ôø Generating {self.config.test_counts['accessibility']} accessibility tests...")
        
        tests = []
        test_file = output_dir / "test_accessibility.py"
        
        test_content = f'''"""
Accessibility Test Suite
========================
Auto-generated accessibility tests for WCAG compliance validation.

Generated: {datetime.now().isoformat()}
Test Count: {self.config.test_counts['accessibility']}
"""

import pytest
from playwright.sync_api import Page, expect


class TestAccessibilitySuite:
    """Comprehensive accessibility test suite"""
    
    def test_page_has_title(self, page: Page):
        """Test that page has a descriptive title"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        title = page.title()
        assert len(title) > 0, "Page must have a title"
        assert len(title) > 3, f"Page title too short: '{{title}}'"
        
        print(f"‚úÖ Page title: {{title}}")
    
    def test_headings_structure(self, page: Page):
        """Test proper heading structure (H1-H6)"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        # Check for H1
        h1_elements = page.locator("h1").all()
        assert len(h1_elements) >= 1, "Page should have at least one H1 element"
        
        if len(h1_elements) > 1:
            print(f"‚ö†Ô∏è Multiple H1 elements found: {{len(h1_elements)}}")
        else:
            print("‚úÖ Single H1 element found")
        
        # Check heading hierarchy
        headings = page.locator("h1, h2, h3, h4, h5, h6").all()
        if headings:
            print(f"‚úÖ {{len(headings)}} headings found")
        else:
            print("‚ö†Ô∏è No headings found")
    
    def test_images_have_alt_text(self, page: Page):
        """Test that images have proper alt text"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        images = page.locator("img").all()
        images_without_alt = 0
        
        for img in images:
            alt_text = img.get_attribute("alt")
            if not alt_text:
                images_without_alt += 1
        
        total_images = len(images)
        if total_images > 0:
            if images_without_alt == 0:
                print(f"‚úÖ All {{total_images}} images have alt text")
            else:
                print(f"‚ö†Ô∏è {{images_without_alt}}/{{total_images}} images missing alt text")
                assert images_without_alt < total_images * 0.5, "Too many images without alt text"
        else:
            print("‚ÑπÔ∏è No images found to test")
    
    def test_form_labels_present(self, page: Page):
        """Test that form inputs have proper labels"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        # Check for form inputs
        inputs = page.locator("input[type='text'], input[type='email'], input[type='password'], textarea").all()
        inputs_without_labels = 0
        
        for input_field in inputs:
            # Check for associated label
            input_id = input_field.get_attribute("id")
            aria_label = input_field.get_attribute("aria-label")
            aria_labelledby = input_field.get_attribute("aria-labelledby")
            
            has_label = False
            
            # Check for label element
            if input_id:
                label = page.locator(f"label[for='{{input_id}}']")
                if label.count() > 0:
                    has_label = True
            
            # Check for aria-label or aria-labelledby
            if aria_label or aria_labelledby:
                has_label = True
            
            if not has_label:
                inputs_without_labels += 1
        
        total_inputs = len(inputs)
        if total_inputs > 0:
            if inputs_without_labels == 0:
                print(f"‚úÖ All {{total_inputs}} form inputs have labels")
            else:
                print(f"‚ö†Ô∏è {{inputs_without_labels}}/{{total_inputs}} inputs without proper labels")
        else:
            print("‚ÑπÔ∏è No form inputs found to test")
    
    def test_keyboard_navigation(self, page: Page):
        """Test keyboard navigation functionality"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        # Test Tab navigation
        focusable_elements = page.locator("a, button, input, textarea, select, [tabindex]").all()
        
        if focusable_elements:
            # Test first few elements
            for i in range(min(5, len(focusable_elements))):
                try:
                    page.keyboard.press("Tab")
                    # Check if something is focused
                    focused = page.evaluate("document.activeElement.tagName")
                    if focused:
                        print(f"‚úÖ Tab {{i+1}}: {{focused}} focused")
                except:
                    print(f"‚ö†Ô∏è Tab navigation issue at step {{i+1}}")
            
            print(f"‚úÖ Keyboard navigation tested on {{min(5, len(focusable_elements))}} elements")
        else:
            print("‚ö†Ô∏è No focusable elements found")
    
    def test_color_contrast_basic(self, page: Page):
        """Basic color contrast validation"""
        url = "{analysis_data.get('url', 'https://example.com')}"
        page.goto(url)
        
        # Get computed styles for text elements
        text_elements = page.locator("p, h1, h2, h3, h4, h5, h6, span, div").all()
        low_contrast_elements = 0
        
        for element in text_elements[:10]:  # Test first 10 text elements
            try:
                if element.is_visible():
                    # Get computed color and background color
                    color = element.evaluate("getComputedStyle(element).color")
                    bg_color = element.evaluate("getComputedStyle(element).backgroundColor")
                    
                    # Simple contrast check (this is basic - full implementation would calculate actual contrast ratios)
                    if color and bg_color:
                        if color == bg_color or (color.startswith("rgb(255") and bg_color.startswith("rgb(255")):
                            low_contrast_elements += 1
                            
            except:
                continue
        
        total_tested = min(10, len(text_elements))
        if total_tested > 0:
            print(f"‚úÖ Color contrast checked on {{total_tested}} elements")
            if low_contrast_elements > 0:
                print(f"‚ö†Ô∏è {{low_contrast_elements}} elements may have contrast issues")
        else:
            print("‚ÑπÔ∏è No text elements found for contrast testing")
'''
        
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_content)
            
        tests.append(str(test_file))
        self.log_message(f"‚úÖ Accessibility tests generated: {test_file}")
        return tests
    
    def generate_load_tests(self, analysis_data: Dict[str, Any], output_dir: Path) -> List[str]:
        """Generate load test cases using Locust"""
        self.log_message("ü¶ó Generating load tests with Locust...")
        
        tests = []
        test_file = output_dir / "locustfile.py"
        
        test_content = f'''"""
Load Test Suite
===============
Auto-generated load tests for performance and scalability validation.

Generated: {datetime.now().isoformat()}
Framework: Locust
"""

from locust import HttpUser, task, between
import random
import time


class WebsiteUser(HttpUser):
    """Load test user behavior simulation"""
    
    wait_time = between(1, 3)  # Wait 1-3 seconds between tasks
    
    def on_start(self):
        """Called when a user starts"""
        self.base_url = "{analysis_data.get('url', 'https://example.com')}"
        print(f"üöÄ Load test user started for: {{self.base_url}}")
    
    @task(3)
    def load_homepage(self):
        """Load the main page (higher weight)"""
        try:
            with self.client.get("/", catch_response=True) as response:
                if response.status_code == 200:
                    response.success()
                    print("‚úÖ Homepage loaded successfully")
                else:
                    response.failure(f"Homepage returned {{response.status_code}}")
        except Exception as e:
            print(f"‚ùå Homepage load failed: {{e}}")
    
    @task(2)
    def load_common_pages(self):
        """Load common pages"""
        common_paths = ["/about", "/contact", "/products", "/services", "/help"]
        path = random.choice(common_paths)
        
        try:
            with self.client.get(path, catch_response=True) as response:
                if response.status_code in [200, 404]:  # 404 is acceptable for non-existent pages
                    response.success()
                    print(f"‚úÖ Page {{path}} tested ({{response.status_code}})")
                else:
                    response.failure(f"Page {{path}} returned {{response.status_code}}")
        except Exception as e:
            print(f"‚ùå Page {{path}} load failed: {{e}}")
    
    @task(1)
    def search_simulation(self):
        """Simulate search functionality"""
        search_terms = ["test", "product", "service", "help", "info"]
        term = random.choice(search_terms)
        
        try:
            with self.client.get(f"/search?q={{term}}", catch_response=True) as response:
                if response.status_code in [200, 404]:
                    response.success()
                    print(f"‚úÖ Search for '{{term}}' completed ({{response.status_code}})")
                else:
                    response.failure(f"Search returned {{response.status_code}}")
        except Exception as e:
            print(f"‚ùå Search for '{{term}}' failed: {{e}}")
    
    @task(1) 
    def form_submission_simulation(self):
        """Simulate form submissions"""
        form_data = {{
            "name": f"TestUser{{random.randint(1, 1000)}}",
            "email": f"test{{random.randint(1, 1000)}}@example.com",
            "message": "This is a load test message"
        }}
        
        try:
            with self.client.post("/contact", data=form_data, catch_response=True) as response:
                if response.status_code in [200, 201, 400, 404]:  # Various acceptable responses
                    response.success()
                    print(f"‚úÖ Form submission completed ({{response.status_code}})")
                else:
                    response.failure(f"Form submission returned {{response.status_code}}")
        except Exception as e:
            print(f"‚ùå Form submission failed: {{e}}")


# Usage:
# locust -f locustfile.py --host={analysis_data.get('url', 'https://example.com')}
# locust -f locustfile.py --host={analysis_data.get('url', 'https://example.com')} --users 10 --spawn-rate 2 --run-time 1m
'''
        
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_content)
            
        tests.append(str(test_file))
        self.log_message(f"‚úÖ Load tests generated: {test_file}")
        return tests
    
    def _load_elements_data(self, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """Load element data from analysis results"""
        # This would load actual element data from the analysis
        # For now, return basic structure
        return {
            'interactive_elements': [],
            'forms': [],
            'images': [],
            'links': []
        }


class AutomatedQAOrchestrator:
    """Main orchestrator for automated QA test generation"""
    
    def __init__(self, log_callback: Optional[Callable[[str], None]] = None):
        self.log_callback = log_callback or print
        self.is_running = False
        self.current_results = {}
        
    def log_message(self, message: str):
        """Log message using callback"""
        self.log_callback(f"[QA-ORCHESTRATOR] {message}")
        
    def run_async(self, analysis_dir: Path, callback: Optional[Callable[[Dict[str, Any]], None]] = None,
                  test_counts: Optional[Dict[str, int]] = None):
        """Run QA automation asynchronously"""
        def worker():
            try:
                self.is_running = True
                results = self.run_sync(analysis_dir, test_counts)
                if callback:
                    callback(results)
            except Exception as e:
                if callback:
                    callback({'error': str(e)})
            finally:
                self.is_running = False
                
        thread = threading.Thread(target=worker, daemon=True)
        thread.start()
        return thread
    
    def run_sync(self, analysis_dir: Path, test_counts: Optional[Dict[str, int]] = None) -> Dict[str, Any]:
        """Run QA automation synchronously"""
        self.log_message("üöÄ Starting automated QA test generation...")
        
        # Create configuration
        config = QATestConfig(test_counts)
        self.log_message(f"üìä Configuration: {config.test_counts}")
        
        # Find analysis directories to process
        analysis_dirs = self._find_analysis_directories(analysis_dir)
        self.log_message(f"üìÅ Found {len(analysis_dirs)} analysis directories to process")
        
        results = {
            'total_pages_processed': len(analysis_dirs),
            'successful_generations': 0,
            'test_files_created': 0,
            'processing_details': [],
            'start_time': datetime.now().isoformat()
        }
        
        # Process each analysis directory
        for i, dir_path in enumerate(analysis_dirs, 1):
            self.log_message(f"üìÑ Processing directory {i}/{len(analysis_dirs)}: {dir_path.name}")
            
            try:
                dir_results = self._process_analysis_directory(dir_path, config)
                results['processing_details'].append(dir_results)
                
                if dir_results['success']:
                    results['successful_generations'] += 1
                    results['test_files_created'] += len(dir_results['generated_files'])
                    
            except Exception as e:
                self.log_message(f"‚ùå Error processing {dir_path}: {e}")
                results['processing_details'].append({
                    'directory': str(dir_path),
                    'success': False,
                    'error': str(e),
                    'generated_files': []
                })
        
        results['end_time'] = datetime.now().isoformat()
        self.log_message(f"‚úÖ QA automation completed: {results['successful_generations']}/{results['total_pages_processed']} successful")
        
        return results
    
    def _find_analysis_directories(self, base_dir: Path) -> List[Path]:
        """Find directories containing analysis data"""
        directories = []
        
        # Check if base_dir itself contains analysis files
        if self._is_analysis_directory(base_dir):
            directories.append(base_dir)
        
        # Look for analysis subdirectories
        for item in base_dir.iterdir():
            if item.is_dir() and self._is_analysis_directory(item):
                directories.append(item)
        
        return directories
    
    def _is_analysis_directory(self, dir_path: Path) -> bool:
        """Check if directory contains analysis data"""
        analysis_files = [
            'metadata.json', 'enhanced_elements.json', 'all_elements.csv',
            'analysis_report.html', 'page_object.py'
        ]
        
        existing_files = sum(1 for file in analysis_files if (dir_path / file).exists())
        return existing_files >= 2  # At least 2 analysis files present
    
    def _process_analysis_directory(self, dir_path: Path, config: QATestConfig) -> Dict[str, Any]:
        """Process a single analysis directory"""
        self.log_message(f"üîÑ Processing: {dir_path.name}")
        
        # Load analysis data
        analysis_data = self._load_analysis_data(dir_path)
        
        # Create generated_tests directory
        tests_dir = dir_path / "generated_tests"
        tests_dir.mkdir(exist_ok=True)
        
        # Generate test suites
        generator = QATestGenerator(config, self.log_callback)
        generated_files = []
        
        try:
            # Generate different types of tests
            generated_files.extend(generator.generate_functional_tests(analysis_data, tests_dir))
            generated_files.extend(generator.generate_negative_tests(analysis_data, tests_dir))
            generated_files.extend(generator.generate_accessibility_tests(analysis_data, tests_dir))
            generated_files.extend(generator.generate_load_tests(analysis_data, tests_dir))
            
            # Create pytest configuration
            self._create_pytest_config(tests_dir)
            
            # Create README for generated tests
            self._create_tests_readme(tests_dir, analysis_data, config)
            
            self.log_message(f"‚úÖ Generated {len(generated_files)} test files for {dir_path.name}")
            
            return {
                'directory': str(dir_path),
                'success': True,
                'generated_files': [Path(f).name for f in generated_files],
                'test_count': config.get_total_tests(),
                'analysis_data': analysis_data
            }
            
        except Exception as e:
            self.log_message(f"‚ùå Error generating tests for {dir_path.name}: {e}")
            return {
                'directory': str(dir_path),
                'success': False,
                'error': str(e),
                'generated_files': []
            }
    
    def _load_analysis_data(self, dir_path: Path) -> Dict[str, Any]:
        """Load analysis data from directory"""
        data = {'url': 'https://example.com'}  # Default
        
        # Try to load metadata
        metadata_file = dir_path / "metadata.json"
        if metadata_file.exists():
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    data.update(metadata)
            except:
                pass
        
        return data
    
    def _create_pytest_config(self, tests_dir: Path):
        """Create pytest configuration file"""
        config_content = '''[tool:pytest]
testpaths = .
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short --strict-markers
markers =
    functional: Functional tests
    negative: Negative tests  
    accessibility: Accessibility tests
    performance: Performance tests
    slow: Slow running tests
'''
        
        config_file = tests_dir / "pytest.ini"
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write(config_content)
    
    def _create_tests_readme(self, tests_dir: Path, analysis_data: Dict[str, Any], config: QATestConfig):
        """Create README file for generated tests"""
        readme_content = f'''# Generated Test Suite

**Generated:** {datetime.now().isoformat()}
**Target URL:** {analysis_data.get('url', 'N/A')}
**Total Tests:** {config.get_total_tests()}

## Test Categories

- **Functional Tests:** {config.test_counts.get('functional', 0)} tests
- **Negative Tests:** {config.test_counts.get('negative', 0)} tests
- **Accessibility Tests:** {config.test_counts.get('accessibility', 0)} tests
- **Load Tests:** Locust-based performance testing

## Running Tests

### Functional, Negative, and Accessibility Tests (Playwright)
```bash
# Install dependencies
pip install pytest playwright

# Install browsers
playwright install

# Run all tests
pytest -v

# Run specific test category
pytest -v test_functional.py
pytest -v test_negative.py  
pytest -v test_accessibility.py
```

### Load Tests (Locust)
```bash
# Install Locust
pip install locust

# Run load tests
locust -f locustfile.py --host={analysis_data.get('url', 'https://example.com')}

# Run headless load test
locust -f locustfile.py --host={analysis_data.get('url', 'https://example.com')} --users 10 --spawn-rate 2 --run-time 1m --headless
```

## Test Configuration

Tests are configured with:
- **Timeout:** 30 seconds per test
- **Retry:** 3 attempts for flaky tests
- **Browser:** Chromium (default)
- **Headless:** True (configurable)

## CI/CD Integration

Add to your pipeline:
```yaml
- name: Run QA Tests
  run: |
    pip install pytest playwright locust
    playwright install
    pytest --junitxml=test-results.xml
```

## Generated Files

- `test_functional.py` - Core functionality validation
- `test_negative.py` - Edge cases and error conditions
- `test_accessibility.py` - WCAG compliance checks
- `locustfile.py` - Performance and load testing
- `pytest.ini` - Test configuration
- `README.md` - This documentation

---
*Auto-generated by QA Orchestrator - Roy Avrahami*
'''
        
        readme_file = tests_dir / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme_content)
    
    def stop(self):
        """Stop the orchestrator"""
        self.is_running = False
        self.log_message("üõë QA Orchestrator stopped")


def create_qa_orchestrator_for_gui(log_callback: Optional[Callable[[str], None]] = None) -> AutomatedQAOrchestrator:
    """Factory function to create QA orchestrator for GUI integration"""
    return AutomatedQAOrchestrator(log_callback)


# Example usage
if __name__ == "__main__":
    def test_log(message):
        print(f"[TEST] {message}")
    
    # Create orchestrator
    orchestrator = create_qa_orchestrator_for_gui(test_log)
    
    # Test with dummy data
    test_dir = Path("./test_analysis")
    test_dir.mkdir(exist_ok=True)
    
    # Create dummy metadata
    (test_dir / "metadata.json").write_text('{"url": "https://example.com", "title": "Test Page"}')
    
    # Run orchestrator
    results = orchestrator.run_sync(test_dir, {'functional': 3, 'negative': 2, 'accessibility': 2})
    print(f"Results: {results}") 